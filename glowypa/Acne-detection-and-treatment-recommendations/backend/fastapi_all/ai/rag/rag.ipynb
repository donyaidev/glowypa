{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community\n",
    "# !pip install langchain\n",
    "# !pip install langchain-chroma\n",
    "# !pip install pypdf\n",
    "# !pip install langchain-google-genai\n",
    "# !pip install --upgrade --quiet  docx2txt\n",
    "# !pip install --upgrade --quiet langchain-community unstructured openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyAhmEYVL47zs_38duN5uH6Lofs1kk8T0ts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"/Users/nhatthuong/Documents/Acne-detection-and-treatment-recommendations/backend/fastapi_all/rag/documents/RESUME_NGUYEN_NHAT_THUONG.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY))\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# def main(question):\n",
    "  \n",
    "#     return response.result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = main(\"What is your name?\")\n",
    "  \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500,google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "system_prompt = (\n",
    "\"Bạn là trợ lý cho các nhiệm vụ trả lời câu hỏi.\"\n",
    "\"Sử dụng các phần sau đây của ngữ cảnh đã thu thập được để trả lời \"\n",
    "\"Câu hỏi. Nếu bạn không biết câu trả lời, hãy nói rằng bạn \"\n",
    "\"Bạn không biết.\"\n",
    "\"trả lời theo format reactjs\"\n",
    "\"Vui lòng trả lời bằng tiếng Việt\"\n",
    "\"\\n\\n\"\n",
    "\"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is new in YOLOv9?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Download Data\n",
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "\n",
    "# Load Data\n",
    "reader = SimpleDirectoryReader(\"./data/10k/\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
    "eval_documents = documents[:20]\n",
    "data_generator = DatasetGenerator.from_documents()\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num = 5)\n",
    "\n",
    "# We will use GPT-4 for evaluating the responses\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define service context for GPT-4 for evaluation\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
    "def evaluate_response_time_and_accuracy(chunk_size):\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # create vector index\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, service_context=service_context\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n",
    "\n",
    "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
    "for chunk_size in [128, 256, 512, 1024, 2048]\n",
    "  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
