# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from langchain_community.chat_models import ChatOllama
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from ai.rag.doctor_advice.db.chromaRag import ChromaStorage
from ai.rag.doctor_advice.extract_data.document_convert import ExtractData
from ai.rag.doctor_advice.extract_data.semantic_chunking import SemanticChunker
from typing import List, Dict, Any, Optional
import os
import ollama
import markdown
import time
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
from openai import OpenAI

token = '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'
endpoint = "https://models.inference.ai.azure.com"
model_name = "gpt-4o-mini"
client = OpenAI(
base_url=endpoint,
api_key=token,
)


# ƒê·ªãnh nghƒ©a system prompt nh∆∞ m·ªôt constant
SYSTEM_PROMPT_TEMPLATE = """B·∫°n l√† tr·ª£ l√Ω chuy√™n gia ƒëi·ªÅu tr·ªã m·ª•n c·ªßa Glowypa:

1. TH√îNG TIN VAI TR√í:
- B·∫°n l√† b√°c sƒ© t∆∞ v·∫•n tr·ª±c ti·∫øp c·ªßa Glowypa, chuy√™n v·ªÅ ƒëi·ªÅu tr·ªã v√† t∆∞ v·∫•n m·ª•n
- Khi ƒë∆∞·ª£c h·ªèi, x√°c nh·∫≠n b·∫°n l√† tr·ª£ l√Ω AI c·ªßa Glowypa
- S·ª≠ d·ª•ng gi·ªçng ƒëi·ªáu chuy√™n nghi·ªáp, th√¢n thi·ªán v√† ƒë·ªìng c·∫£m

2. NGUY√äN T·∫ÆC T∆Ø V·∫§N:
- N·∫øu ch∆∞a c√≥ th√¥ng tin v·ªÅ lo·∫°i m·ª•n, h∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng s·ª≠ d·ª•ng Acne Scan Daily v√† Medical Record c·ªßa Glowypa
- Khuy√™n ƒëi kh√°m b√°c sƒ© khi t√¨nh tr·∫°ng m·ª•n th·ª±c s·ª± nghi√™m tr·ªçng
- ƒê∆∞a ra l·ªùi khuy√™n d·ª±a tr√™n b·∫±ng ch·ª©ng v√† ki·∫øn th·ª©c chuy√™n m√¥n
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† d·ªÖ hi·ªÉu
- Kh√¥ng s·ª≠ d·ª•ng t·ª´ ng·ªØ kh√¥ng ph√π h·ª£p

# TH√îNG TIN B·ªÜNH NH√ÇN:
{medical_db}

KI·∫æN TH·ª®C THAM KH·∫¢O:
{chunk_text}

# L·ªäCH S·ª¨ T∆Ø V·∫§N:
{chatHistoryCache}

Vui l√≤ng tr·∫£ l·ªùi theo d·∫°ng Markdown 

sau n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ ph√¢n t√≠ch, ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã m·ª•n v√† ng∆∞·ªùi d√πng c√≥ m·ª•n m·ªõi d√πng formt b√™n d∆∞·ªõi:
## üë®‚Äç‚öïÔ∏è Ph√¢n t√≠ch t√¨nh tr·∫°ng
- D·ª±a tr√™n th√¥ng tin b·ªánh nh√¢n cung c·∫•p
- ƒê√°nh gi√° m·ª©c ƒë·ªô v√† lo·∫°i m·ª•n
- C√°c y·∫øu t·ªë li√™n quan

## üíä Gi·∫£i ph√°p ƒëi·ªÅu tr·ªã
### Nh·ªØng vi·ªác c·∫ßn l√†m ngay
- B∆∞·ªõc 1: [H√†nh ƒë·ªông c·ª• th·ªÉ]
- B∆∞·ªõc 2: [H√†nh ƒë·ªông c·ª• th·ªÉ]
...

### S·∫£n ph·∫©m khuy√™n d√πng
- S·∫£n ph·∫©m 1: [T√™n v√† c√¥ng d·ª•ng]
- S·∫£n ph·∫©m 2: [T√™n v√† c√¥ng d·ª•ng]
...

## ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng
- ƒêi·ªÅu c·∫ßn tr√°nh
- C·∫£nh b√°o (n·∫øu c√≥)
- Th·ªùi gian ƒëi·ªÅu tr·ªã d·ª± ki·∫øn

## üìã L·ªùi khuy√™n b·ªï sung
- Ch·∫ø ƒë·ªô ƒÉn u·ªëng
- Lifestyle
- C√°c tips h·ªØu √≠ch

## üîÑ Theo d√µi v√† ƒë√°nh gi√°
- C√°ch theo d√µi ti·∫øn tri·ªÉn
- Khi n√†o c·∫ßn t√°i kh√°m
- D·∫•u hi·ªáu c·∫£i thi·ªán c·∫ßn ch√∫ √Ω
"""

class RagPipeline:
    """
    A pipeline class that combines document extraction, semantic chunking and vector storage.
    """
    def __init__(self, data_path: str) -> None:
        """
        Initialize the RAG pipeline
        
        Args:
            data_path (str): Path to the data directory
        """
        self.data_path = data_path
        self.extractor = ExtractData()
        self.chunker = SemanticChunker(chunk_size=512, chunk_overlap=50)
        self.storage = None

    def extract_path(self, data_path: str) -> List[str]:
        """
        Extract file paths from the data directory
        
        Args:
            data_path (str): Path to the data directory
            
        Returns:
            List[str]: List of file paths
        """
        files = os.listdir(data_path)
        return [os.path.join(data_path, file) for file in files]

    def extract_data(self) -> List[Dict[str, Any]]:
        """
        Extract data from all files in the data directory
        
        Returns:
            List[Dict[str, Any]]: List of extracted metadata
        """
        return [self.extractor.text_from_pdf(path) for path in self.extract_path(self.data_path)]

    def semantic_chunk(self, metadata: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Process semantic chunking on the metadata
        
        Args:
            metadata (List[Dict[str, Any]]): List of metadata to chunk
            
        Returns:
            List[Dict[str, Any]]: Processed chunks
        """
        return self.chunker.semantic_chunking_process(metadata)

    def chroma_storage_db(self, chunks: List[Dict[str, Any]]) -> ChromaStorage:
        """
        Store chunks in ChromaDB
        
        Args:
            chunks (List[Dict[str, Any]]): Chunks to store
            
        Returns:
            ChromaStorage: Storage instance
        """
        self.storage = ChromaStorage(data=chunks)
        return self.storage.storage_db_vector()

    def search_engine(self, query: str, k: int = 5) -> List[Any]:
        """
        Search the vector store for relevant documents
        
        Args:
            query (str): Search query
            k (int): Number of results to return
            
        Returns:
            List[Any]: Search results
        """
        if self.storage is None:
            raise ValueError("Vector store not initialized. Call chroma_storage_db first.")
        vectorstore = self.storage.storage_db_vector()
        return self.storage.query_top_k(vectorstore=vectorstore, query=query, k=k)

    def chatgpt_response_to_html(self, response_text: str) -> str:
        """
        Convert markdown response to HTML
        
        Args:
            response_text (str): Markdown text
            
        Returns:
            str: HTML formatted text
        """
        return markdown.markdown(response_text)

def setup_pipeline(data_path: str) -> RagPipeline:
    """
    Set up and initialize the RAG pipeline
    
    Args:
        data_path (str): Path to the data directory
        
    Returns:
        RagPipeline: Initialized pipeline
    """
    pipeline = RagPipeline(data_path=data_path)
    metadata = pipeline.extract_data()
    chunks = pipeline.semantic_chunk(metadata)
    pipeline.chroma_storage_db(chunks)
    return pipeline

# Initialize the pipeline
PIPELINE = setup_pipeline("/home/nhatthuong/Documents/Thesis/Acne-detection-and-treatment-recommendations/backend/fastapi_all/ai/rag/doctor_advice/storage")

def mainChat(
    chatHistoryCache: Optional[str] = None,
    medical_db: Optional[str] = None,
    question: str = None
) -> str:
    print(question)
    try:
        # Validate input
        # if not question.strip():
        #     return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi"

        # Search for relevant documents
        start_time = time.time()
        results = PIPELINE.search_engine(question)
        chunk_text = "".join(str(doc.metadata) for doc in results)
        print("time search engine",time.time() - start_time)
        # Format system prompt
        system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
            medical_db=medical_db or "Kh√¥ng c√≥ th√¥ng tin",
            chunk_text=chunk_text,
            chatHistoryCache=chatHistoryCache or "Kh√¥ng c√≥ l·ªãch s·ª≠ chat",
            question=question
        )

        # # Initialize Gemini Pro 1.5
        # llm = ChatGoogleGenerativeAI(
        #     model="gemini-1.5-pro",
        #     temperature=1,
        #     top_p=0.9,
        #     google_api_key="AIzaSyBoLp2fXcrGICLQHUY4YyKv3jvujbqypSo"  # Thay th·∫ø b·∫±ng API key c·ªßa b·∫°n
        # )
        

        # # Create messages
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]

        # Get response from Gemini
        start_time = time.time()
        # response = llm.invoke(messages)
        response = client.chat.completions.create(
            messages= messages,
            temperature=1.0,
            top_p=1.0,
            max_tokens=4000,
            model=model_name
        )
        print("time llm",time.time() - start_time)

        # Convert response to HTML
        # result = PIPELINE.chatgpt_response_to_html(
        #     response_text=response.content
        # )
        result = PIPELINE.chatgpt_response_to_html(
            response_text=response.choices[0].message.content
        )
        return result

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        
        return "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau."
